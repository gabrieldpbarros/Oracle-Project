import streamlit as st

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI

VALID_ARCH_TYPES = [
    "Site", "Youtube", "PDF", "CSV", "TXT"
]

VALID_PROVIDERS = {"Groq":
                        {"models": ["llama-3.3-70b-versatile", "llama-3.1-8b-instant", "openai/gpt-oss-120b", "openai/gpt-oss-20b" , "gemma2-9b-it", "deepseek-r1-distill-llama-70b"],
                         "chat": ChatGroq},
                   "OpenAI":
                        {"models": ["gpt-4o-mini"],
                         "chat": ChatOpenAI},
                    "Google":
                        {"models" : ["gemini-2.5-flash", "gemini-2.5-pro"],
                         "chat": ChatGoogleGenerativeAI}}

MEMORY = ConversationBufferMemory()
# Demonstra칞칚o b치sica de mem칩ria
# MEMORY.chat_memory.add_user_message("Hello, AI!")
# MEMORY.chat_memory.add_ai_message("Hello, human!")

def load_model(provider: str, model: str, api_key: str):
    """
        CARREGA O MODELO ESPECIFICADO PELOS PAR츽METROS

        Insere as informa칞칫es passadas  fun칞칚o em uma vari치vel padronizada e a salva na mem칩ria do Streamlit
    """
    # Padroniza칞칚o para qualquer modelo
    chat = VALID_PROVIDERS[provider]["chat"](model=model, api_key=api_key) # carrega a classe respons치vel pela inicializa칞칚o do modelo e fornece os par칙metros requeridos
    st.session_state["chat"] = chat # salva na mem칩ria do Streamlit o modelo utilizado, evitando a necessidade de retornar a vari치vel


def chat_page():
    """
        ESTRUTURA DE CHAT

        Engloba o t칤tulo de sauda칞칚o e o funcionamento do chat (mem칩ria e mensagens aparecendo)
    """
    # T칤tulo que aparece na p치gina principal (par칙metro 'divider' coloca uma divis칩ria abaixo do t칤tulo)
    st.header("游 Welcome to your personal Oracle!", divider=True)

    # Session state 칠 a mem칩ria do Streamlit 
    chat_model = st.session_state.get("chat") # puxa da mem칩ria o chat
    memory = st.session_state.get("memory", MEMORY)
    for message in memory.buffer_as_messages:
        chat = st.chat_message(message.type) # formata칞칚o de chat (role)
        chat.markdown(message.content) # escreve no chat criado (conte칰do)

    user_input = st.chat_input("Write here")
    if user_input:
        # Parte respons치vel por fazer o texto aparecer sequencialmente no chat
        chat = st.chat_message("human") # cria uma nova janela do usu치rio
        chat.markdown(user_input) # mostra a mensagem que o usu치rio enviou
        chat = st.chat_message("ai") # cria uma nova janela da IA
        answer = chat.write_stream(chat_model.stream(user_input)) # escreve a mensagem em stream

        memory.chat_memory.add_user_message(user_input) # salva a mensagem do usu치rio (langchain)
        # answer = chat_model.invoke(user_input).content # armazena a mensagem que ser치 passada para o modelo (OLD)
        memory.chat_memory.add_ai_message(answer) # salva a mensagem do modelo (langchain)
        st.session_state["memory"] = memory # atualiza a mem칩ria do Streamlit

def sidebar():
    """
        ESTRUTURA DE SIDEBAR

        Inclui duas abas principais, uma de upload de arquivos e outra de sele칞칚o do modelo.

        A primeira aba inclui a sele칞칚o do formato que ser치 utilizado e caixa de texto/bot칚o
        de upload para anexar a fonte.

        A segunda aba engoloba a se칞칚o do modelo que o usu치rio vai escolher para compor o
        Or치culo.
    """
    tabs = st.tabs(["Archive Upload", "Model Selection"])
    # Editamos a primeira aba (upload de arquivos)
    with tabs[0]:
        arch_type = st.selectbox("Select the archive type", VALID_ARCH_TYPES)
        # Sele칞칚o de formato de envio dos arquivos
        if (arch_type == "Site" or arch_type == "Youtube"):
            arch = st.text_input("URL")
        if (arch_type == "PDF"):
            arch = st.file_uploader("Upload", type=[".pdf"])
        if (arch_type == "CSV"):
            arch = st.file_uploader("Upload", type=[".csv"])
        if (arch_type == "TXT"):
            arch = st.file_uploader("Upload", type=[".txt"])

    # Editamos a segunda aba (sele칞칚o do LLM)
    with tabs[1]:
        provider = st.selectbox("Select the provider", VALID_PROVIDERS.keys())
        model = st.selectbox("Select the model", VALID_PROVIDERS[provider]["models"])
        api_key = st.text_input(
            f"Insert your API key for {provider}",
            value=st.session_state.get(f"api_key_{provider}"))

        # Armazena na mem칩ria do Streamlit o valor inserido na caixa de texto da chave da API
        st.session_state[f"api_key_{provider}"] = api_key # salva uma vari치vel chamada "api_key_{provedor}" na mem칩ria

    if st.button("Initialize Oracle", use_container_width=True): # o segundo par칙metro apenas ajusta o bot칚o para o mesmo tamanho das selectboxes
        load_model(provider, model, api_key)

def main():
    chat_page()
    with st.sidebar: # tudo que estiver dentro deste m칠todo vai ser exibido na sidebar do Streamlit
        sidebar() # modelo que constru칤mos

if __name__ == "__main__":
    main()

# algum bug acontece que reinicia os texboxes quando converso com o modelo, corrigir isso
# CORRE칂츾O: o st.rerun() no condicional do input do usu치rio, na fun칞칚o chat_page, por algum motivo estava reiniciando o que aparecia na sidebar